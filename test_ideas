#!/usr/bin/env node

// Self-contained file to try ideas

import { createReadStream, createWriteStream } from 'node:fs'
import { Transform } from 'node:stream'
import { pipeline } from 'node:stream/promises'
import { basename } from 'node:path'

import { glob } from 'glob'
import createSplitStream from 'split2'
import createBunzipper from 'unbzip2-stream'
import chalk from 'chalk'

let [, , ...inputFiles] = process.argv

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

// turn 2 arrays (keys, values) into an object
const obzip = (keys, values) => keys.reduce((a, k, i) => ({ ...a, [k]: values[i] }), {})

// process 1 line: output an object with all values encoded as javascript strings
function parseRow(line, { info: { group, name, date }, fields, headers, exportMode, primaryKeys }) {
  const data = line.split('\x01')

  // this means there were too many values for headers
  if (headers.length !== data.length) {
    console.error(chalk.red('ERR'), name, obzip(headers, data))
    return undefined
  }

  return undefined
}

// make your stream here
export default function createEPFParserStream() {
  let headers
  let fields
  let primaryKeys
  let exportMode
  let lineNum = 0
  let info
  const stream = new Transform({
    objectMode: true,

    transform(chunk, encoding, callback) {
      const line = chunk.toString()
      lineNum++

      // 1st line is header
      if (!headers) {
        const [junk, headersT] = line.split('#')
        headers = headersT.split('\x01')
        // pull lots of info from tar-header
        const r = /^([a-z]+)([0-9]{4})([0-9]{2})([0-9]{2})\/([a-z_]+)/
        const [, group, year, month, day, name] = r.exec(junk)
        info = { group, name, date: new Date(year, month - 1, day) }
        console.error(chalk.yellow('info'), info)
        callback()
      } else {
        // handle comments
        if (line.startsWith('#')) {
          let [k, v] = line.substr(1).split(':')
          if (k === 'primaryKey') {
            primaryKeys = v.split('\x01')
            console.error(chalk.yellow('primaryKeys'), primaryKeys)
          } else if (k === 'dbTypes') {
            fields = obzip(headers, v.split('\x01'))
            console.error(chalk.yellow('fields'), fields)
          } else if (k === 'exportMode') {
            exportMode = v
            console.error(chalk.yellow('exportMode'), exportMode)
          } else if (k !== '#legal') {
            console.error(chalk.yellow(k), v)
          }
          return callback()
        }

        return callback(null, parseRow(line, { info, fields, headers, exportMode, primaryKeys }))
      }
    }
  })
  return stream
}

for (const file of inputFiles) {
  const name = basename(file, '.tbz')
  console.error(chalk.blue(name))
  const reader = createReadStream(file)
  const unbz = createBunzipper()
  const splitter = createSplitStream('\x02\n')
  const epf2csv = createEPFParserStream(name)
  const writer = createWriteStream(file.replace('.tbz', '.csv'))
  await pipeline(reader, unbz, splitter, epf2csv, writer)
}
