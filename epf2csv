#!/usr/bin/env node

// util to convert EPF files to more friendly CSV

import { mkdir } from 'node:fs/promises'
import { glob } from 'glob'
import chalk from 'chalk'

import { getInfo, createBzStream, getSqlColumns, epf2csv } from './lib/epf.js'

const { yellow } = chalk

// show time in a nicer format
function formatMillisecondsToTime(milliseconds) {
  const totalSeconds = Math.floor(milliseconds / 1000)
  const hours = Math.floor(totalSeconds / 3600)
  const minutes = Math.floor((totalSeconds % 3600) / 60)
  const seconds = totalSeconds % 60

  const formattedHours = String(hours).padStart(2, '0')
  const formattedMinutes = String(minutes).padStart(2, '0')
  const formattedSeconds = String(seconds).padStart(2, '0')

  return `${formattedHours}:${formattedMinutes}:${formattedSeconds}`
}

let [, , ...inputFiles] = process.argv

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

const outDir = 'data/csv'
await mkdir(outDir, { recursive: true })

const start = performance.now()
let last = start

for (const f of inputFiles) {
  const info = await getInfo(f)
  console.error(yellow(info.name), info)
  const outFile = `${outDir}/${info.name}.csv`
  const stats = await epf2csv(info, f, outFile)
  console.error('Skipped', stats.skipped.length, 'rows')

  console.error(`took`, formatMillisecondsToTime(performance.now() - last), 'to complete', info.name)
  last = performance.now()

  // output duckdb command to import this, I log so you can pipe it directly to duck
  console.log(`CREATE TABLE ${info.name} AS SELECT * FROM read_csv('${outFile}',columns={${getSqlColumns(info)}});`)
}

const end = performance.now()
console.error(`took`, formatMillisecondsToTime(end - start), 'to complete', inputFiles.length, 'tables')
