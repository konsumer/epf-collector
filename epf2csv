#!/usr/bin/env node

// util to convert EPF files to more friendly CSV

import { spawn } from 'node:child_process'
import { Transform, Readable, Writable } from 'node:stream'
import { promisify } from 'node:util'
import { pipeline } from 'node:stream/promises'
import { createWriteStream } from 'node:fs'
import { mkdir } from 'node:fs/promises'
import { basename } from 'node:path'

import { format as createCsvStream } from '@fast-csv/format'
import createSplitter from 'split2'
import { glob } from 'glob'
import chalk from 'chalk'

// fast bzip2 stream (requires pbzip2 in path)
// inMem will do the operation in memory (a bit faster, at cost of memory)
function createBzStream(file, inMem=true) {
  const options = ['-cd']
  if (inMem) {
    options.push('-r')
  }
  const s = spawn('pbzip2', [...options, file])
  return s.stdout
}

// get info from EPF header
// not using here, but eventuially this will setup sql
async function getInfo(file) {
  const bz = createBzStream(file)
  let info = {}
  // just grabs first chunk and parses header, then quits
  const epf = new Transform({
    transform(chunk, encoding, callback) {
      const lines = chunk.toString().split('\x02\n')
      const [tarHeader, fieldsS] = lines[0].split('#')
      info.dbNames = fieldsS.split('\x01')
      const r = /^([a-z]+)([0-9]{4})([0-9]{2})([0-9]{2})\/([a-z_]+)/
      const [, group, year, month, day, name] = r.exec(tarHeader)
      info.group = group
      info.name = name
      info.date = new Date(year, month - 1, day)
      for (const line of lines.slice(1)) {
        if (line.startsWith('#')) {
          const [k, v] = line.substr(1).split(':')
          if (['primaryKey', 'dbTypes'].includes(k)) {
            info[k] = v.split('\x01')
          }
          if (['exportMode'].includes(k)) {
            info[k] = v
          }
        }
      }
      info.primaryKeys = info.primaryKey.filter((k) => info.dbNames.includes(k))
      info.types = info.dbNames.reduce((a, c, i) => ({ ...a, [c]: info.dbTypes[i] }), {})
      delete info.primaryKey
      delete info.dbNames
      delete info.dbTypes

      callback('done')
    }
  })
  // is there a better way to close immediately when I'm done?
  try {
    await pipeline(bz, epf)
  } catch (e) {}
  return info
}

// output SQL to create table
// not using here, but eventuially this will setup sql
function getCreateSql(info) {
  const { name, types, primaryKeys } = info
  const fields = []
  for (const [field, type] of Object.entries(types)) {
    fields.push(`${field} ${type.replace('LONGTEXT', 'TEXT')}`)
  }
  if (primaryKeys?.length) {
    fields.push(`PRIMARY KEY(${primaryKeys.join(', ')})`)
  }
  return `CREATE OR REPLACE TABLE ${name} (${fields.join(', ')});`
}

// outputs format for duck's read_csv
function getSqlColumns(info) {
  const fields = []
  for (const [field, type] of Object.entries(info.types)) {
    fields.push(`'${field}': '${type.replace('LONGTEXT', 'TEXT')}'`)
  }
  return fields.join(', ')
}

function formatMillisecondsToTime(milliseconds) {
  const totalSeconds = Math.floor(milliseconds / 1000);
  const hours = Math.floor(totalSeconds / 3600);
  const minutes = Math.floor((totalSeconds % 3600) / 60);
  const seconds = totalSeconds % 60;

  const formattedHours = String(hours).padStart(2, '0');
  const formattedMinutes = String(minutes).padStart(2, '0');
  const formattedSeconds = String(seconds).padStart(2, '0');

  return `${formattedHours}:${formattedMinutes}:${formattedSeconds}`;
}

// very simple format: delimiter is \1, newlines are escaped
async function epf2csv(info, epfFile, outFile) {
  const headers = Object.keys(info.types)

  const bz = createBzStream(epfFile)
  const split = createSplitter('\x02\n')
  const csv = createCsvStream()

  let linenum=0
  const skipped =[]

  const converter = new Transform({
    objectMode: true,
    transform(chunk, encoding, callback) {
      linenum++
      const line = chunk.toString('utf8')
      if (line.startsWith('#') || linenum === 1) {
        return callback()
      }
     
      const row = line.split('\x01')
      if (row.length !== headers.length) {
        // skip bad lines
        skipped.push(linenum)
        return callback()
      }

      callback(null, row)
    }
  })
  const writer = createWriteStream(outFile)
  await pipeline(bz, split, converter, csv, writer)

  return {skipped, linenum}
}

let [, , ...inputFiles] = process.argv

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

const outDir='data/csv'
await mkdir(outDir, { recursive: true })

const start = performance.now()
let last = start

for (const f of inputFiles) {
  const info = await getInfo(f)
  console.error(chalk.yellow(info.name), info)
  const outFile = `${outDir}/${info.name}.csv`
  const stats = await epf2csv(info, f, outFile)
  console.error('Skipped', stats.skipped.length, 'rows')
  
  console.error(`took`, formatMillisecondsToTime(performance.now() - last), 'to complete', info.name)
  last = performance.now()
  
  // output duckdb command to import this, I log so you can pipe it directly to duck
  console.log(`CREATE TABLE ${info.name} AS SELECT * FROM read_csv('${outFile}',columns={${getSqlColumns(info)}});`)
}

const end = performance.now()
console.error(`took`, formatMillisecondsToTime(end - start), 'to complete', inputFiles.length, 'tables')
