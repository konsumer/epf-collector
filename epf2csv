#!/usr/bin/env node

// util to convert EPF files to more friendly CSV

import { mkdir } from 'node:fs/promises'
import { glob } from 'glob'
import chalk from 'chalk'

import { getInfo, createBzStream, getSqlColumns, epf2csv } from './lib/epf.js'
import { formatTime } from './lib/utils.js'

const { yellow } = chalk

let [, , ...inputFiles] = process.argv

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

const outDir = 'data/csv'
await mkdir(outDir, { recursive: true })

const start = performance.now()
let last = start

for (const f of inputFiles) {
  const info = await getInfo(f)
  console.error(yellow(info.name), info)
  const outFile = `${outDir}/${info.name}.csv`
  const stats = await epf2csv(info, f, outFile)
  console.error('Skipped', stats.skipped.length, 'rows')

  console.error(`took`, formatTime((performance.now() - last) / 1000), 'to complete', info.name)
  last = performance.now()

  // output duckdb command to import this, I log so you can pipe it directly to duck
  console.log(`CREATE TABLE ${info.name} AS SELECT * FROM read_csv('${outFile}',columns={${getSqlColumns(info)}});`)
}

const end = performance.now()
console.error(`took`, formatTime((end - start) / 1000), 'to complete', inputFiles.length, 'tables')
