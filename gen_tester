#!/usr/bin/env node

// I want faster test-data to parse (to figure out parsing issues)

import { pipeline } from 'node:stream/promises'
import { createReadStream, createWriteStream } from 'node:fs'
import { mkdir, rename } from 'node:fs/promises'
import { basename } from 'node:path'
import { Transform } from 'node:stream'

import { promisify } from 'node:util'
import childProcess from 'node:child_process'

import { glob } from 'glob'
import createBunzipper from 'unbzip2-stream'
import chalk from 'chalk'

import createByteSplitter from './lib/ByteSplitterStream.js'
import createBinaryProgressStream from './lib/BinaryProgressStream.js'
import { getFileSize } from './lib/fs.js'

const exec = promisify(childProcess.exec)

// turn 2 arrays (keys, values) into an object
export const obzip = (keys, values) => keys.reduce((a, k, i) => ({ ...a, [k]: values[i] }), {})

// parse a single row (Buffer) with lots of info
export function parseRow(chunk, { headers }) {
  const data = chunk.toString().split('\x01')
  if (data.length !== headers.length) {
    return false
  }
  return true
}

function createEPFParser() {
  return new Transform({
    transform(chunk, encoding, callback) {
      this.currentLine ||= 0
      this.currentLine++

      if (this.currentLine === 1) {
        // first line is header
        const [junk, headersT] = chunk.toString().split('#')
        this.headers = headersT.split('\x01')
        // pull lots of info from tar-header
        const r = /^([a-z]+)([0-9]{4})([0-9]{2})([0-9]{2})\/([a-z_]+)/
        const [, group, year, month, day, name] = r.exec(junk)
        this.info = { group, name, date: new Date(year, month - 1, day) }
        this.emit('info', ['header', this.info])
        this.push(chunk)
        this.push('\x02\n')
      } else if (chunk[0] === 35) {
        // # comment

        let [k, v] = chunk.toString().substr(1).split(':')
        if (k === 'primaryKey') {
          this.primaryKeys = v.split('\x01')
          this.emit('info', ['primaryKeys', this.primaryKeys])
        } else if (k === 'dbTypes') {
          this.fields = obzip(this.headers, v.split('\x01'))
          this.emit('info', ['fields', this.fields])
        } else if (k === 'exportMode') {
          this.exportMode = v
          this.emit('info', ['exportMode', v])
        } else if (k !== '#legal') {
          this.emit('info', ['legal', v])
        } else {
          this[k] = v
          this.emit('info', [k, v])
        }
        this.push(chunk)
        this.push('\x02\n')
      } else {
        // data
        const row = parseRow(chunk, this)
        if (!row) {
          this.push(chunk)
          this.push('\x02\n')
        }
      }
      callback()
    }
  })
}

let [, , ...inputFiles] = process.argv

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

await mkdir('data/test', { recursive: true })

for (const file of inputFiles) {
  const name = basename(file, '.tbz')
  console.log(chalk.blue(name))
  const outFile = `data/test/${name}`
  const size = await getFileSize(file)

  const reader = createReadStream(file)
  const progger = createBinaryProgressStream(size, process.stderr, { updateMore: true })
  const unbzipper = createBunzipper()
  const splitter = createByteSplitter('\x02\n')
  const parser = createEPFParser()
  const writer = createWriteStream(outFile)

  await pipeline(reader, progger, unbzipper, splitter, parser, writer)
  // make space for the progress-bar

  console.log(chalk.yellow('compressing'))
  await exec(`bzip2 "${outFile}"`)
  await rename(`${outFile}.bz2`, `${outFile}.tbz`)
}
