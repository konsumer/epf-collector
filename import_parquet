#!/usr/bin/env node

// CLI util to turn an EPF file into a CSV file and then import it into a DuckDB database.

import { Transform, PassThrough } from 'node:stream'
import { pipeline } from 'node:stream/promises'
import { createReadStream, createWriteStream, writeFileSync } from 'node:fs'
import { mkdir } from 'node:fs/promises'
import { basename } from 'node:path'

import p from 'parquetjs'
import { glob } from 'glob'
import createBunzipper from 'unbzip2-stream'
import chalk from 'chalk'

import createByteSplitter from './lib/ByteSplitterStream.js'
import createEPFParser from './lib/EPFPaserStream.js'
import createBinaryProgressStream from './lib/BinaryProgressStream.js'
import { getFileSize } from './lib/fs.js'
import * as tables from './lib/tables_parquet.js'

// it's a stupid commonjs module
const { ParquetSchema, ParquetTransformer } = p

let [, , ...inputFiles] = process.argv

// I may put these in options, later
const outDir = 'data/parquet'
const errorChunksDir=false

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

await mkdir(outDir, {recursive: true})

if (errorChunksDir) {
  await mkdir(errorChunksDir, {recursive: true})
}

// just puts objects into correct shape for parquet-encoder
function createParquetStream(name) {
  const processStream = new Transform({
    objectMode: true,
    transform(chunk, encoding, callback) {
      const value = {}
      for (const [field, { type }] of Object.entries(tables[name])) {
        value[field] = chunk[field] === '' ? null : value[field] = chunk[field]
      }
      callback(null, { value })
    }
  })
  return processStream
}


export const application_price = {
  export_date: { type: "INT64", optional: true },
  application_id: { type: "INT64", optional: false },  // Primary key
  retail_price: { type: "DOUBLE", optional: true },
  currency_code: { type: "UTF8", optional: true },
  storefront_id: { type: "INT32", optional: false }    // Primary key
};


for (const file of inputFiles) {
  const name = basename(file, '.tbz')
  console.log(chalk.blue(name))
  const parquetFile = `${outDir}/${name}.parquet`
  console.error(chalk.blue('parquet'), parquetFile)

  const size = await getFileSize(file)

  const reader = createReadStream(file)
  const progger = createBinaryProgressStream(size, process.stderr, { updateMore: true })
  const unbzipper = createBunzipper()
  const splitter = createByteSplitter('\x02\n')
  const parser = createEPFParser()
  const parqueter = createParquetStream(name)

  const outStream = new ParquetTransformer(new ParquetSchema({ value: { fields: tables[name] } }))

  const writer = createWriteStream(parquetFile)

  let skipped = 0
  let count = 0
  const info = {}

  parser.on('skipped', ({ chunk, lineNumber }) => {
    skipped++
    const row = chunk.toString().split('\x01')
    console.error(chalk.red('\nskipped'), lineNumber, row.length, parser.headers.length)
    if (errorChunksDir) {
      writeFileSync(`${errorChunksDir}/${name}-${lineNumber}.bin`, chunk)
    }
  })
  parser.on('parsed', ({ row, chunk, lineNumber }) => {
    count++
  })
  parser.on('info', ([key, value]) => {
    if (key !== 'legal') {
      info[key] = value
    }
  })

  await pipeline(reader, progger, unbzipper, splitter, parser, parqueter, outStream, writer)
  // make space for the progress-bar
  console.error()

  console.error(chalk.yellow('info'), info)
  console.error(chalk.green('good'), count, 'rows')
  console.error(chalk.yellow('skipped'), skipped, 'rows')
}
