#!/usr/bin/env node

// CLI util to turn an EPF file into a CSV file and then import it into a DuckDB database.

import { pipeline } from 'node:stream/promises'
import { createReadStream, createWriteStream, writeFileSync } from 'node:fs'
import { mkdir } from 'node:fs/promises'
import { basename } from 'node:path'
import { unlink } from 'fs/promises'

import { glob } from 'glob'
import createBunzipper from 'unbzip2-stream'
import { stringify as createCsvStream } from 'csv-stringify'
import chalk from 'chalk'
import { DuckDBInstance } from '@duckdb/node-api'
import hexy from 'hexy'

import createByteSplitter from './lib/ByteSplitterStream.js'
import createEPFParser from './lib/EPFPaserStream.js'
import createBinaryProgressStream from './lib/BinaryProgressStream.js'
import { getFileSize } from './lib/fs.js'

// generate the SQL for table from info
function getCreateSql(table, types, primaryKeys) {
  const fields = []
  const primaries = []
  for (const [field, type] of Object.entries(types)) {
    fields.push(`${field} ${type.replace('LONGTEXT', 'TEXT')}`)
    // genre has mismatch with primaryKeys!
    if (primaryKeys.includes(field)) {
      primaries.push(field)
    }
  }
  if (primaries?.length) {
    fields.push(`PRIMARY KEY(${primaries.join(', ')})`)
  }
  return `CREATE OR REPLACE TABLE ${table} (${fields.join(', ')});`
}

let [, , ...inputFiles] = process.argv

// I may put these in options, later
const dbFile = 'data/epf.duckdb'
const outDir = 'data/csv'
const deleteCsvFile = false

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

const instance = await DuckDBInstance.create(dbFile)
const db = await instance.connect()
await mkdir(outDir, {recursive: true})

for (const file of inputFiles) {
  const name = basename(file, '.tbz')
  console.log(chalk.blue(name))
  const csvFile = `${outDir}/${name}.csv`
  console.error(chalk.blue('csv'), csvFile)

  const size = await getFileSize(file)

  const reader = createReadStream(file)
  const progger = createBinaryProgressStream(size, process.stderr, { updateMore: true })
  const unbzipper = createBunzipper()
  const splitter = createByteSplitter('\x02\n')
  const parser = createEPFParser()
  const csver = createCsvStream({ header: true, quoted_string: true })
  const writer = createWriteStream(csvFile)

  let skipped = 0
  let count = 0
  const info = {}

  parser.on('skipped', ({ chunk, lineNumber }) => {
    skipped++
    const row = chunk.toString().split('\x01')
    console.error(chalk.red('\nskipped'), lineNumber, row.length, parser.headers.length)
    // writeFileSync(`data/chunks/${name}-${lineNumber}.bin`, chunk)
  })
  parser.on('parsed', ({ row, chunk, lineNumber }) => {
    count++
  })
  parser.on('info', ([key, value]) => {
    if (key !== 'legal') {
      info[key] = value
    }
  })

  await pipeline(reader, progger, unbzipper, splitter, parser, csver, writer)
  // make space for the progress-bar
  console.error()

  console.error(chalk.yellow('info'), info)
  console.error(chalk.green('good'), count, 'rows')
  console.error(chalk.yellow('skipped'), skipped, 'rows')

  console.error(chalk.blue('importing'), dbFile)
  await db.run(getCreateSql(parser.info.name, parser.fields, parser.primaryKeys))

  if (await getFileSize(csvFile)) {
    await db.run(`INSERT OR REPLACE INTO ${parser.info.name} SELECT * FROM read_csv('${csvFile}', header=true, nullstr='');`)
  }
  if (deleteCsvFile) {
    console.error(chalk.blue('deleting'), csvFile)
    await unlink(csvFile)
  }
}
