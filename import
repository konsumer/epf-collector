#!/usr/bin/env node

// CLI util to turn an EPF file into a CSV file and then import it into a DuckDB database.

import { pipeline } from 'node:stream/promises'
import { createReadStream, createWriteStream } from 'node:fs'
import { basename } from 'node:path'
import { unlink } from 'fs/promises'

import { glob } from 'glob'
import createBunzipper from 'unbzip2-stream'
import { stringify as createCsvStream } from 'csv-stringify'
import chalk from 'chalk'
import { DuckDBInstance } from '@duckdb/node-api'

import createByteSplitter from './lib/ByteSplitterStream.js'
import createEPFParser from './lib/EPFPaserStream.js'
import createBinaryProgressStream from './lib/BinaryProgressStream.js'
import { getFileSize } from './lib/fs.js'

// generate the SQL for table from info
function getCreateSql(table, types, primaryKeys) {
  const fields = []
  for (const [field, type] of Object.entries(types)) {
    fields.push(`${field} ${type.replace('LONGTEXT', 'TEXT')}`)
  }
  if (primaryKeys?.length) {
    fields.push(`PRIMARY KEY(${primaryKeys.join(', ')})`)
  }
  return `CREATE OR REPLACE TABLE ${table} (${fields.join(', ')});`
}

let [, , dbFile = 'data/epf.duckdb', ...inputFiles] = process.argv

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

const instance = await DuckDBInstance.create(dbFile)
const db = await instance.connect()

for (const file of inputFiles) {
  console.log(chalk.blue(basename(file, '.tbz')))
  const csvFile = file.replace('.tbz', '.csv')
  console.error(chalk.blue('csv'), csvFile)

  const size = await getFileSize(file)

  const reader = createReadStream(file)
  const progger = createBinaryProgressStream(size, process.stderr, { updateMore: true })
  const unbzipper = createBunzipper()
  const splitter = createByteSplitter('\x02\n')
  const parser = createEPFParser()
  const csver = createCsvStream({ header: true, quoted_string: true })
  const writer = createWriteStream(csvFile)

  let skipped = 0
  parser.on('skipped', ({ chunk, lineNumber }) => {
    skipped++
    const row = chunk
      .toString()
      // .replace(/\x01\x01/g, '\x01')
      .split('\x01')
    console.error(chalk.red('\nskipped'), lineNumber, row.length, parser.headers.length, row)
  })

  // this info is also in the stream object, but it's nice to print as it comes in
  parser.on('info', ([key, value]) => {
    console.error(chalk.yellow(key), value)
  })

  await pipeline(reader, progger, unbzipper, splitter, parser, csver, writer)
  // make space for the progress-bar
  console.error()

  console.error(chalk.yellow('skipped'), skipped, 'rows')

  console.error(chalk.blue('importing'), dbFile)
  await db.run(getCreateSql(parser.info.name, parser.fields, parser.primaryKeys))

  if (await getFileSize(csvFile)) {
    await db.run(`INSERT OR REPLACE INTO ${parser.info.name} SELECT * FROM read_csv('${csvFile}', header=true, nullstr='');`)
  }
  await unlink(csvFile)
}
