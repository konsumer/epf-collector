#!/usr/bin/env node

// Self-contained file to try ideas

import { createReadStream, createWriteStream } from 'node:fs'
import { unlink } from 'node:fs/promises'
import { Transform } from 'node:stream'
import { pipeline } from 'node:stream/promises'
import { basename } from 'node:path'

import { DuckDBInstance } from '@duckdb/node-api'
import { glob } from 'glob'
import createSplitStream from 'split2'
import createBunzipper from 'unbzip2-stream'
import chalk from 'chalk'
import { stringify } from 'csv-stringify'
import h from 'hexy'

const { hexy } = h

let [, , ...inputFiles] = process.argv

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

const instance = await DuckDBInstance.create('data/epf.duckdb')
const db = await instance.connect()

// turn 2 arrays (keys, values) into an object
const obzip = (keys, values) => keys.reduce((a, k, i) => ({ ...a, [k]: values[i] }), {})

// process 1 EPF line: output an object with all values encoded as javascript strings
function parseRow(line, { info: { group, name, date }, fields, headers, exportMode, primaryKeys }) {
  let data = line.split('\u0001')

  if (data.length !== headers.length) {
    const fixed = {}
    let i = 0
    for (const [field, type] of Object.entries(fields)) {
      if (type === 'TEXT' || type === 'LONGTEXT') {
        // TODO: find other fields
        break
      } else {
        // fill in fields before problematic fields
        fixed[field] = data[i]
      }
      i++
    }
    console.error(chalk.red('mismatch'), 'got', data.length, 'expected', headers.length, fixed)
    return fixed
  }

  return obzip(headers, data)
}

// receives a stream of EPF lines, outputs objects for each row
export default function createEPFParserStream(processFunction = parseRow) {
  const stream = new Transform({
    objectMode: true,

    transform(chunk, encoding, callback) {
      const line = chunk.toString('utf8')
      stream.lineNum ||= 0
      stream.lineNum++

      // 1st line is header
      if (!stream.headers) {
        const [junk, headersT] = line.split('#')
        stream.headers = headersT.split('\x01')
        // pull lots of info from tar-header
        const r = /^([a-z]+)([0-9]{4})([0-9]{2})([0-9]{2})\/([a-z_]+)/
        const [, group, year, month, day, name] = r.exec(junk)
        stream.info = { group, name, date: new Date(year, month - 1, day) }
        console.error(chalk.yellow('info'), stream.info)
        return callback()
      } else {
        // handle comments
        if (line.startsWith('#')) {
          let [k, v] = line.substr(1).split(':')
          if (k === 'primaryKey') {
            stream.primaryKeys = v.split('\x01')
            console.error(chalk.yellow('primaryKeys'), stream.primaryKeys)
          } else if (k === 'dbTypes') {
            stream.fields = obzip(stream.headers, v.split('\x01'))
            console.error(chalk.yellow('fields'), stream.fields)
          } else if (k === 'exportMode') {
            stream.exportMode = v
            console.error(chalk.yellow('exportMode'), stream.exportMode)
          } else if (k !== '#legal') {
            console.error(chalk.yellow(k), v)
          } else {
            stream[k] = v
          }
          return callback()
        }
        return callback(null, processFunction(line, stream))
      }
    }
  })
  return stream
}

function getCreateSql(table, types, primaryKeys) {
  const fields = []
  for (const [field, type] of Object.entries(types)) {
    fields.push(`${field} ${type.replace('LONGTEXT', 'TEXT')}`)
  }
  if (primaryKeys?.length) {
    fields.push(`PRIMARY KEY(${primaryKeys.join(', ')})`)
  }
  return `CREATE OR REPLACE TABLE ${table} (${fields.join(', ')});`
}

for (const file of inputFiles) {
  const name = basename(file, '.tbz')
  const csvFile = file.replace('.tbz', '.csv')

  console.error(chalk.blue(name))

  const reader = createReadStream(file)
  const unbzipper = createBunzipper()
  const splitter = createSplitStream('\x02\n')
  const parser = createEPFParserStream()
  const csver = stringify({ header: true, quoted_string: true })
  const writer = createWriteStream(csvFile)

  await pipeline(reader, unbzipper, splitter, parser, csver, writer)

  await db.run(getCreateSql(name, parser.fields, parser.primaryKeys))
  await db.run(`INSERT OR REPLACE INTO ${name} SELECT * FROM read_csv('${csvFile}', header=true, nullstr='');`)
  // await unlink(csvFile)
}
