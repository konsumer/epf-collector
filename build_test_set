#!/usr/bin/env node

// This is a self-contained script to generate a test-set (minimal EPF with a few good records & bad from full EPFs)

import { Transform } from 'node:stream'
import { pipeline } from 'node:stream/promises'
import { basename } from 'node:path'
import { exec as _exec } from 'node:child_process'
import { promisify } from 'node:util'
import { createReadStream, createWriteStream } from 'node:fs'
import { unlink, mkdir } from 'node:fs/promises'

import { glob } from 'glob'
import createSplitStream from 'split2'
import createBunzipper from 'unbzip2-stream'
import chalk from 'chalk'

let [,,dirOutput='data/test', ...inputFiles] = process.argv

if (!inputFiles?.length) {
  inputFiles = await glob('data/epf/**/*.tbz')
}

const exec = promisify(_exec)

await mkdir(dirOutput, {recursive: true})

// minimal transformer that takes EPF rows and outputs objects (with header)
export default function createEPFParserStream(quitfast) {
  let headers
  let goodCount = 0
  const stream = new Transform({
    objectMode: true,
    transform(chunk, encoding, callback) {
      const line = chunk.toString()

      // 1st line
      if (!headers) {
        headers = line.split('#').pop().split('\x01')
        console.log(chalk.yellow('headers'), headers.join(','))
        callback(null, `#${line.split('#').pop()}\x02\n`)
      } else {
        if (line.startsWith('#')) {
          // output original comments
          callback(null, `${line}\x02\n`)
        } else {
          // check the data
          let l = line.split('\x01')

          // if headers do not match, it's bad
          if (l.length !== headers.length) {
            console.log(chalk.red('bad'), l.join(','))
            callback(null, `${line}\x02\n`)
          } else {
            // it's good: we only want 10 of these
            if (goodCount++ < 10) {
              console.log(chalk.green('good'), l.join(','))
              callback(null, `${line}\x02\n`)
            } else {
              if (quitfast) {
                stream.destroy()
              } else {
                 callback()
              }
            }
          }
        }
      }
    }
  })
  return stream
}

for (const file of inputFiles) {
  const name = basename(file, '.tbz')
  console.log(name)
  const rawFile = `${dirOutput}/${name}`

  const reader = createReadStream(file)
  const unbz = createBunzipper()
  const splitter = createSplitStream('\x02\n')
  const parser = createEPFParserStream(!['application', 'application_detail'].includes(name))
  const writer = createWriteStream(rawFile)

  try {
    await pipeline(reader, unbz, splitter, parser, writer)
  } catch (e) { }
  await exec(`tar -cjf "${name}.tbz" "${name}"`, {cwd: dirOutput})
  await unlink(rawFile)
}
